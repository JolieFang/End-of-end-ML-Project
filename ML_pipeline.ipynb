{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7073baee",
   "metadata": {},
   "source": [
    "**Steps to be followed**\n",
    "1. Importing necessary libraries\n",
    "2. Creating S3 bucket (to save the models into AWS sagemaker - scaleable)\n",
    "3. Mapping train and test data in S3 (map the output path of the model on S3)\n",
    "4. Mapping the path of the models in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7836eec4",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfeb856",
   "metadata": {},
   "source": [
    "#### 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6bbc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import sagemaker\n",
    "import boto3 #read the S3 buckets if it's public\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "from sagemaker.session import s3_input, Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a6284",
   "metadata": {},
   "source": [
    "#### 2. Create a new S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26ef0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'atp-tennis-prediction'\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a68486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 bucket\n",
    "# Source: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-creating-buckets.html\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region is None:\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da55f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the function to create S3 bucket\n",
    "create_bucket(bucket_name, my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49aa0b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets:\n",
      "  atp-tennis-prediction\n",
      "  dev-terraform-state-bucket-1\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the list of existing buckets\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b433ec8",
   "metadata": {},
   "source": [
    "#### 3. Mapping the output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e33ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://atp-tennis-prediction/xgboost-as-a-built-in-algo/output\n"
     ]
    }
   ],
   "source": [
    "# Set an output path where the trained model will be saved\n",
    "prefix = 'xgboost-as-a-built-in-algo'\n",
    "output_path = 's3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d51fbd",
   "metadata": {},
   "source": [
    "#### 4. Downloading the Dataset and Save it on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bd20474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded bank_clean.csv.\n",
      "Success: Data loaded into dataframe.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "try:\n",
    "    urllib.request.urlretrieve (\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \"bank_clean.csv\")\n",
    "    print('Success: downloaded bank_clean.csv.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "\n",
    "try:\n",
    "    model_data = pd.read_csv('./bank_clean.csv',index_col=0)\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f4043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 61) (12357, 61)\n"
     ]
    }
   ],
   "source": [
    "### Train Test split\n",
    "\n",
    "import numpy as np\n",
    "train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data))])\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef0f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving Train And Test Into Buckets\n",
    "## We start with Train Data\n",
    "import os\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], \n",
    "                                                axis=1)], \n",
    "                                                axis=1).to_csv('train.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48909568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Into Buckets\n",
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "s3_input_test = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/test'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b51446",
   "metadata": {},
   "source": [
    "## Model Building & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e2b750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the version depending on your preference.\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name,\n",
    "                         'xgboost',\n",
    "                         repo_version='1.0-1')\n",
    "\n",
    "#container = sagemaker.image_uris.retrieve(my_region,'xgboost',version='1.0-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f4c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "# Hyperparameter tuning should be donee separately\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":50\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b85b8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:train_max_run has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:train_use_spot_instances has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:train_max_wait has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:train_volume_size has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m5.2xlarge', #2x large --> faster\n",
    "                                          train_volume_size=5, # 5 GB \n",
    "                                          output_path=output_path,\n",
    "                                          train_use_spot_instances=True, # important parameters to limit the building time/reduce the building hours\n",
    "                                          train_max_run=300, # in seconds\n",
    "                                          train_max_wait=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0225a731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-17 12:44:46 Starting - Starting the training job...\n",
      "2022-04-17 12:44:48 Starting - Launching requested ML instancesProfilerReport-1650199486: InProgress\n",
      "......\n",
      "2022-04-17 12:46:16 Starting - Preparing the instances for training......\n",
      "2022-04-17 12:47:07 Downloading - Downloading input data\n",
      "2022-04-17 12:47:07 Training - Downloading the training image...\n",
      "2022-04-17 12:47:42 Uploading - Uploading generated training model\n",
      "2022-04-17 12:47:42 Completed - Training job completed\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[12:47:30] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[12:47:30] 12357x59 matrix with 729063 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 28831 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 12357 rows\u001b[0m\n",
      "\u001b[34m[12:47:30] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.10079#011validation-error:0.10528\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.09968#011validation-error:0.10456\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.10017#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.09989#011validation-error:0.10310\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.09996#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.09906#011validation-error:0.10261\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.09930#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.09951#011validation-error:0.10261\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.09920#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.09871#011validation-error:0.10294\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.09868#011validation-error:0.10294\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.09868#011validation-error:0.10326\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.09854#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.09892#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.09850#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.09844#011validation-error:0.10326\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.09857#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.09799#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.09816#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.09857#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.09830#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.09826#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.09847#011validation-error:0.10399\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.09833#011validation-error:0.10407\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.09812#011validation-error:0.10415\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.09812#011validation-error:0.10399\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.09774#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.09781#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.09781#011validation-error:0.10391\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.09778#011validation-error:0.10367\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.09781#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.09771#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.09743#011validation-error:0.10391\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.09753#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.09767#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.09757#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.09757#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.09736#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.09750#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.09733#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.09705#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.09701#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.09712#011validation-error:0.10407\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.09698#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.09733#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.09736#011validation-error:0.10367\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.09746#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.09736#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.09712#011validation-error:0.10334\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.09712#011validation-error:0.10318\u001b[0m\n",
      "Training seconds: 48\n",
      "Billable seconds: 20\n",
      "Managed Spot Training savings: 58.3%\n"
     ]
    }
   ],
   "source": [
    "# Input is from the S3 bucket\n",
    "\n",
    "estimator.fit({'train': s3_input_train,'validation': s3_input_test}) # The input value is the path of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384cd53",
   "metadata": {},
   "source": [
    "### Deploy Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ce4741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = estimator.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type='ml.m4.xlarge')\n",
    "\n",
    "# xgb_predictor = estimator.deploy(\n",
    "#             initial_instance_count=1,\n",
    "#             instance_type='ml.m4.xlarge')\n",
    "\n",
    "# instance_count helps to have parallel instances (to get faster response)\n",
    "# xlarge is selected so that the response time is less"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe623d1d",
   "metadata": {},
   "source": [
    "**Prediction of the Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ca5c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a library to serialize a tabular dataset to be passed to the model\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "# Drop the dependent feature from test data and convert into an array\n",
    "test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb8d9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .content_type everytime we use csv_serializer\n",
    "xgb_predictor.CONTENT_TYPE = 'text/csv' # set the data type for an inference\n",
    "xgb_predictor.serializer = CSVSerializer() # set the serializer type\n",
    "# xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5adc59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test data\n",
    "# Decoding is required because when we are doing the prediction, the format was encoded, so we need to decode\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "331bf64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12357,)\n"
     ]
    }
   ],
   "source": [
    "# Take the first part of the particular data\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3abeec76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05214286, 0.05660191, 0.05096195, ..., 0.03436061, 0.02942475,\n",
       "       0.03715819])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "243580f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Classification Rate: 89.7%\n",
      "\n",
      "Predicted      No Purchase    Purchase\n",
      "Observed\n",
      "No Purchase    91% (10785)    34% (151)\n",
      "Purchase        9% (1124)     66% (297) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a confusion matrix\n",
    "\n",
    "# Crosstab \n",
    "cm = pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]; fn = cm.iloc[1,0]; tp = cm.iloc[1,1]; fp = cm.iloc[0,1]; p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
    "print(\"Observed\")\n",
    "\n",
    "# Based on the purchase/no purchase, check the classification matrix\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"No Purchase\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Purchase\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6feab0",
   "metadata": {},
   "source": [
    "### Deleting the End Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete endpoints of the estimator\n",
    "# sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "\n",
    "# # Specify bucket to delete\n",
    "# bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "\n",
    "# # Everything will be deleted\n",
    "# bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1a72a",
   "metadata": {},
   "source": [
    "### Backup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "001a9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import a library to serialize a tabular dataset to be passed to the model\n",
    "# from sagemaker.serializers import CSVSerializer\n",
    "# from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "# # Drop the dependent feature from test data and convert into an array\n",
    "# test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n",
    "\n",
    "# class FMSerializer(CSVSerializer):\n",
    "#     def serialize(self, data):\n",
    "#         c = {'instances': []}\n",
    "#         for row in data:\n",
    "#             c['instances'].append({'features': row.tolist()})\n",
    "#         return csv.dumps(c)\n",
    "\n",
    "\n",
    "# xgb_predictor = estimator.deploy(\n",
    "#             initial_instance_count=1,\n",
    "#             instance_type='ml.m4.xlarge',\n",
    "#             serializer=FMSerializer(),\n",
    "#             deserializer=JSONDeserializer()\n",
    "# )\n",
    "\n",
    "# # instance_count helps to have parallel instances (to get faster response)\n",
    "# # xlarge is selected so that the response time is less"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
